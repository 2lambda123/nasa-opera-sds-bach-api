# import os
# from datetime import timedelta

# from accountability_api.api_utils.utils import ElasticsearchResultDictWrapper
# from accountability_api.configuration_obj import ConfigurationObj
# from flask_restx import Namespace, Resource, reqparse
# from accountability_api.api_utils import query, JOBS_ES

# api = Namespace(
#     "Preprocessor Workflow",
#     path="/preprocessorWorkflowsAPI",
#     description="Preprocessor Workflow Query",
# )

# parser = reqparse.RequestParser()
# parser.add_argument(
#     "startDateTime", dest="start_dt", type=str, location="args", default=""
# )
# parser.add_argument("endDateTime", dest="end_dt", type=str, location="args", default="")
# # parser.add_argument("startOrbit", dest="start_orbit", type=str, location='args', default='')
# # parser.add_argument("endOrbit", dest="end_orbit", type=str, location='args', default='')


# @api.route("/<path:workflow_name>")
# class PreprocessorWorkflow(Resource):
#     @staticmethod
#     def __create_dict(each):
#         src = each["_source"]
#         restructured_dict = {
#             "workflow_id": src["payload_id"] if "payload_id" in src else "NA",
#             "workflow_name": src["job_id"] if "job_id" in src else "NA",
#             "status": src["status"] if "status" in src else "NA",
#             "short_error_msg": src["short_error"] if "short_error" in src else "NA",
#             "error_msg": src["error"] if "error" in src else "NA",
#             "half_orbit_id": "unknown",
#             "progress": "unknown",
#         }
#         if "job" in src and "job_info" in src["job"]:
#             job_info = src["job"]["job_info"]
#             restructured_dict["start_time"] = (
#                 job_info["time_start"] if "time_start" in job_info else "-"
#             )
#             restructured_dict["end_time"] = (
#                 job_info["time_end"] if "time_end" in job_info else "-"
#             )

#         dotted_result = ElasticsearchResultDictWrapper(src)
#         restructured_dict["processing_mode"] = dotted_result.get_val(
#             "job.params.runconfig.processing_mode", default="-"
#         )
#         raw_inputs = dotted_result.get_val("context.product_paths", default=[])
#         if raw_inputs is None:
#             raw_inputs = []
#         elif isinstance(raw_inputs, str):
#             raw_inputs = [os.path.basename(raw_inputs)]
#         else:
#             raw_inputs = [os.path.basename(k) for k in raw_inputs]
#         restructured_dict["input_names"] = raw_inputs
#         return restructured_dict

#     @api.expect(parser)
#     def get(self, workflow_name):
#         """
#         Getting Preprocessor workflows

#         :return:
#         """

#         """
# workflow id : id
# parent id
# workflow name
# half orbit
# progress
# status : status
# error message
# workflow start time : @timestamp
# workflow duration (minutes) : job.job_info.duration
# Pid : hide?
# Node name : node ip address
# processing mode: metadata.ProcessingMode
# Trigger Value : was generated by task id and names
# """
#         args = parser.parse_args()
#         config = ConfigurationObj()
#         config.get_item("JOB_CONTAINER_NAME", default="container-sds-smap_smap-sciflo")
#         start_dt, end_dt = args["start_dt"], args["end_dt"]
#         source_include = [
#             "payload_id",
#             "job_id",
#             "status",
#             "job.job_info.time_start",
#             "job.job_info.time_end",
#             "context.product_paths",
#             "error",
#             "short_error",
#             "job.params.runconfig.processing_mode",
#         ]
#         pp_query = {
#             "query": {
#                 "bool": {
#                     "must": [
#                         {
#                             "range": query.construct_range_object(
#                                 "@timestamp", start_dt, end_dt
#                             )
#                         },
#                         {
#                             "prefix": {
#                                 "job.container_image_name": config.get_item(
#                                     "JOB_CONTAINER_NAME",
#                                     default="container-sds-smap_smap-sciflo",
#                                 )
#                             }
#                         },
#                         # {
#                         #     "terms": {
#                         #         "tags.untouched": PREPROCESSOR_WORKFLOW_NAMES if workflow_name == 'all' else [workflow_name]
#                         #     }
#                         # }
#                     ],
#                     "must_not": [{"match": {"tags": "trigger"}}],
#                 }
#             },
#             "sort": [
#                 {"@timestamp": "desc"},
#             ],
#         }
#         if workflow_name == "others":
#             pp_query["query"]["bool"]["must"].append(
#                 {
#                     "bool": {
#                         "must_not": [
#                             {"match": {"tags": k}}
#                             for k in PREPROCESSOR_WORKFLOW_DICT.values()
#                         ]
#                     }
#                 }
#             )
#             pass
#         elif workflow_name != "all":
#             pp_query["query"]["bool"]["must"].append(
#                 {"match": {"tags": PREPROCESSOR_WORKFLOW_DICT[workflow_name]}}
#             )
#         pp_response = query.run_query_with_scroll(
#             es=JOBS_ES,
#             index="job_status-current",
#             body=pp_query,
#             _source_include=source_include,
#         )
#         restructured_dict = [
#             self.__create_dict(each) for each in pp_response["hits"]["hits"]
#         ]
#         return restructured_dict, 200
